{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One can use these samples to enter when code asks for an input.\n",
    "\n",
    "sample1 - The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\n",
    "\n",
    "sample2 = The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\n",
    "\n",
    "sample3 = We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm used to solve the problem.\n",
    "### Term Frequency - Inverse Document Frequency\n",
    "Term Frequency or TF is the frequency of the word occuring in the certain document.\n",
    "Inverse Document Frequency or IDF is the inverse of frequency of the word occuring across all documents.\n",
    "Finally we multiply TF and IDF of every word to get TFIDF value.\n",
    "\n",
    "This algorithm is used for converting the text into numerics but also certain word are given more importance than others.\n",
    "The IDF function returns higher values for words that occur rarely across all documents,\n",
    "but the TF function returns higher values for wors that occur frequently in a given document.\n",
    "Thus, the words occuring highly in one document but rarely across other documents are given higher values and vice-versa.\n",
    "\n",
    "This algorithm thus helps us to get scores of the words in the documents that are useful to determine similarity between documents based on high impact words. Although we are using this algorithm for 2 documents, we can easily scale this algorithm for multiple documents as well.\n",
    "\n",
    "### The metric used is cosine similarity.\n",
    "Finally after getting the TFIDF for every word in the document, we essentially convert the document into an array of numerical values corresponding to the words.\n",
    "\n",
    "The cosine similarity helps to get a score between 0 and 1 for similarity between these 2 arrays. Score of 1 means highly similar and score of 0 means highly unsimilar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm code block.\n",
    "\n",
    "This has codes to take inputs from user, three functions that perform TF, IDF & TFIDF operations and a function to calculate cosine similarity for 2 1-D arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter document 1: The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\n",
      "\n",
      "Enter document 2: The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\n",
      "\n",
      "\n",
      "Cosine similarity between document 1 and document 2:  0.613\n",
      "A score of 1 means highly similar and score of 0 means highly unsimilar.\n"
     ]
    }
   ],
   "source": [
    "# Taking the input from the user.\n",
    "\n",
    "doc1 = str(input(\"Enter document 1: \"))\n",
    "doc2 = str(input(\"\\nEnter document 2: \"))\n",
    "\n",
    "# Since grammatical correctness of the samples is unknown, I am considering only the words for comparison.\n",
    "\n",
    "# This regular expression will remove all punctuation marks and only alphanumerics and _ and spaces would be retained.\n",
    "doc1 = re.sub(r'[^\\w\\s]',\"\",doc1)\n",
    "doc2 = re.sub(r'[^\\w\\s]',\"\",doc2)\n",
    "\n",
    "#split so each word have their own string\n",
    "doc1 = doc1.split(\" \")\n",
    "doc2= doc2.split(\" \")\n",
    "\n",
    "# I join them to get rid of duplicates. This will help us get the total vocabulary from the 2 entered documents.\n",
    "vocabulary = set(doc1).union(set(doc2))\n",
    "\n",
    "# Creating dictionaries to keep the count of occurence of every word within the document.\n",
    "# Initially count of all words is zero.\n",
    "CountWordDoc1 = dict.fromkeys(vocabulary, 0) \n",
    "CountWordDoc2 = dict.fromkeys(vocabulary, 0)\n",
    "\n",
    "# Increasing the count if word exists in the document.\n",
    "for word in doc1:\n",
    "    CountWordDoc1[word]+=1\n",
    "    \n",
    "for word in doc2:\n",
    "    CountWordDoc2[word]+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TF is the term-frequency or the frequency of every word in the document.\n",
    "# It is calculated as (Number of occurences in the document) / (Total number of words in the document)\n",
    "\n",
    "def termFrequency( CountWordDoc, wordList):\n",
    "    '''\n",
    "    Args:\n",
    "    \n",
    "    CountWordDoc - The dictionary that keeps count of occurences ofevery word in the document.\n",
    "    wordList - The list of words in the document; Input document after splitting.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Dictionary with key value pair of word and its term-frequency.\n",
    "    '''\n",
    "    \n",
    "    tf = {}\n",
    "    totalWords = len(wordList)\n",
    "    for word, count in CountWordDoc.items():\n",
    "        tf[word] = count/float(totalWords)\n",
    "    return tf\n",
    "\n",
    "# Calculating the TF for every document\n",
    "tfDoc1 = termFrequency(CountWordDoc1, doc1)\n",
    "tfDoc2 = termFrequency(CountWordDoc2, doc2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# IDF is the Inverse Document Frequency. That means, the inverse of occurence of the word across documents.\n",
    "# It is calculated as (Total Number of Documents) / (Number of Documents in which the word occurs).\n",
    "# A logarithm is taken to scale down the values.\n",
    "\n",
    "def InverseDocFrequency(docList):\n",
    "    '''\n",
    "    Args:\n",
    "    \n",
    "    docList - List of dictionaries that keep the word count for every document.\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "    A Dictionary that keeps track of idf for each word.\n",
    "    '''\n",
    "    \n",
    "    N = len(docList)\n",
    "    \n",
    "    # Now word occurence is counted across all the documents.\n",
    "    counts = [i+j for i,j in zip(list(docList[0].values()),list(docList[1].values()))]\n",
    "    \n",
    "    # A new dict is created to store these values for each word.\n",
    "    idf = dict(zip(list(docList[0].keys()),counts))\n",
    "\n",
    "    # Applying the idf formula for every word. +1 is done to avoid division by zero.\n",
    "    for word, count in idf.items():\n",
    "        idf[word] = 1 + (np.log((N+1)/(count+1)))\n",
    "        \n",
    "    return idf\n",
    "\n",
    "# Calculate the idf.\n",
    "idf = InverseDocFrequency([CountWordDoc1, CountWordDoc2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Multiplying TF and IDF of every word\n",
    "# It is done as TF*IDF.\n",
    "\n",
    "def TFIDF(tfDoc, idf):\n",
    "    '''\n",
    "    Args:\n",
    "    \n",
    "    tfDoc - Dictionary of Term Frequencies calculated for every word in the document.\n",
    "    idf   - Dictionary of Inverse Document Frequencies calculated for all the words in the vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Dictionary containing Tf*IDF for every word in the document.\n",
    "    '''\n",
    "    \n",
    "    tfidf = {}\n",
    "    for word, val in tfDoc.items():\n",
    "        tfidf[word] = val*idf[word]\n",
    "    return tfidf\n",
    "\n",
    "# Calculate the TF-IDF.\n",
    "tfidfDoc1 = TFIDF(tfDoc1, idf)\n",
    "tfidfDoc2 = TFIDF(tfDoc2, idf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculating the similarity using the cosine metrics.\n",
    "def cosine_similarity(tfidfDocument1, tfidfDocument2):\n",
    "    '''\n",
    "    Args:\n",
    "    \n",
    "    tfidfDocument1 - dictionary of TFIDF values for words in the document 1.\n",
    "    tfidfDocument2 - dictionary of TFIDF values for words in the document 2.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    Float value between 0 and 1 representing the cosine similarity between document1 and document2.\n",
    "    '''\n",
    "    \n",
    "    a = list(tfidfDocument1.values())\n",
    "    b = list(tfidfDocument2.values())\n",
    "\n",
    "    # calculate Cosine Similarity\n",
    "    # norm of list a is the square-root of sum of squares of all values in a.\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    \n",
    "    return np.round(cos_sim,3)\n",
    "\n",
    "print(\"\\n\\nCosine similarity between document 1 and document 2: \", cosine_similarity(tfidfDoc1,tfidfDoc2))\n",
    "print(\"A score of 1 means highly similar and score of 0 means highly unsimilar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
